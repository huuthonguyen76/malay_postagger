{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malay POS tagger benchmarking\n",
    "\n",
    "This notebook benchmarks multiple Part Of Speech tagger models trained on the PAN Localization Project's [POS tagged corpus](http://www.panl10n.net/english/outputs/Indonesia/UI/0802/UI-1M-tagged.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "sents_tagged = pickle.load(open('../data/tagged/malay_pos_tagged.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validation evaluation function\n",
    "#from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import metrics\n",
    "import numpy as np \n",
    "\n",
    "def cross_val(tagger, train_set, n_folds=10):    \n",
    "    \n",
    "    cv = KFold(len(train_set), n_folds, shuffle=True)\n",
    "    scores = {\"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"F1-Score\": []}\n",
    "\n",
    "    for train_idx, test_idx in cv:\n",
    "        train = [train_set[i] for i in train_idx]\n",
    "        test = [train_set[i] for i in test_idx]\n",
    "\n",
    "        #Train\n",
    "        tagger.train(train)\n",
    "        X_test = [[token for token,_ in test_sent] for test_sent in test]\n",
    "        y_test = [tag for test_sent in test for _,tag in test_sent]\n",
    "\n",
    "        #Predict\n",
    "        tagged_pred = tagger.tag_sents(X_test)\n",
    "        y_pred = [tag for pred_sent in tagged_pred for _,tag in pred_sent]\n",
    "\n",
    "        scores[\"Accuracy\"].append(metrics.accuracy_score(y_test, y_pred))\n",
    "        scores[\"Precision\"].append(metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "        scores[\"Recall\"].append(metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "        scores[\"F1-Score\"].append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    \n",
    "    ave_scores = {k+'_mean':np.mean(v) for k,v in scores.items()}\n",
    "    for metric, score in ave_scores.items():\n",
    "        print('{}: {}'.format(metric, score))\n",
    "    return ave_scores\n",
    "\n",
    "def evaluateTagger(tagger, test_sents):\n",
    "    scores = {}\n",
    "    X_test = [[token for token,_ in test_sent] for test_sent in test_sents]\n",
    "    y_test = [tag for test_sent in test_sents for _,tag in test_sent]\n",
    "\n",
    "    \n",
    "    tagged_pred = tagger.tag_sents(X_test)\n",
    "    y_pred = [tag for pred_sent in tagged_pred for _,tag in pred_sent]\n",
    "    y_pred = ['' if tag is None else tag for tag in y_pred]\n",
    "    \n",
    "    scores[\"Accuracy\"] = metrics.accuracy_score(y_test, y_pred)\n",
    "    scores[\"Precision\"] = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "    scores[\"Recall\"] = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "    scores[\"F1-Score\"] = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    for metric, score in scores.items():\n",
    "        print('{}: {}'.format(metric, score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Sequential Backoff Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backoff_tagger(tagged_sents, tagger_classes, backoff=None):\n",
    "    '''\n",
    "    Convenience function to build sequential backoff taggers\n",
    "    '''\n",
    "    if not backoff:\n",
    "        backoff = tagger_classes[0](tagged_sents)\n",
    "        del tagger_classes[0]\n",
    " \n",
    "    for cls in tagger_classes:\n",
    "        tagger = cls(tagged_sents, backoff=backoff)\n",
    "        backoff = tagger\n",
    " \n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UBT tagger:\n",
      "F1-Score: 0.9799887930062702\n",
      "Recall: 0.9646927828573805\n",
      "Accuracy: 0.9646927828573805\n",
      "Precision: 0.9965080818984777\n",
      "\n",
      "Suffix3-UBT:\n",
      "F1-Score: 0.9904810864166493\n",
      "Recall: 0.9875579251812255\n",
      "Accuracy: 0.9875579251812255\n",
      "Precision: 0.9934852661892651\n",
      "\n",
      "Prefix3-UBT:\n",
      "F1-Score: 0.9875094555375102\n",
      "Recall: 0.9844069801287175\n",
      "Accuracy: 0.9844069801287175\n",
      "Precision: 0.9908071922231153\n",
      "\n",
      "Suffix3-Prefix3-UBT:\n",
      "F1-Score: 0.9882445002838782\n",
      "Recall: 0.9859916929630433\n",
      "Accuracy: 0.9859916929630433\n",
      "Precision: 0.9906529492113243\n",
      "\n",
      "Regex-Suffix3-Prefix3-UBT:\n",
      "F1-Score: 0.9882268358886409\n",
      "Recall: 0.9859916929630433\n",
      "Accuracy: 0.9859916929630433\n",
      "Precision: 0.9906177142676877\n"
     ]
    }
   ],
   "source": [
    "#Train test split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_sents, test_sents = train_test_split(sents_tagged)\n",
    "\n",
    "tagger_scores = {}\n",
    "#Unigram, bigram and trigram seq. backoff\n",
    "ubt_tagger = backoff_tagger(train_sents, [nltk.tag.UnigramTagger, nltk.tag.BigramTagger, nltk.tag.TrigramTagger])\n",
    "print('\\nUBT tagger:')\n",
    "tagger_scores['ubt'] = evaluateTagger(ubt_tagger, test_sents)\n",
    "\n",
    "#UBT, affix tagger suffix=3\n",
    "suffix3_tagger = nltk.tag.AffixTagger(train_sents)\n",
    "s3ubt_tagger = nltk.tag.DefaultTagger('NN')\n",
    "s3ubt_tagger._taggers = ubt_tagger._taggers + suffix3_tagger._taggers\n",
    "print('\\nSuffix3-UBT:')\n",
    "tagger_scores['s3ubt'] = evaluateTagger(s3ubt_tagger, test_sents)\n",
    "\n",
    "#UBT, affix tagger prefix=3\n",
    "prefix3_tagger = nltk.tag.AffixTagger(train_sents, affix_length=3)\n",
    "p3ubt_tagger = nltk.tag.DefaultTagger('NN')\n",
    "p3ubt_tagger._taggers = ubt_tagger._taggers + prefix3_tagger._taggers\n",
    "print('\\nPrefix3-UBT:')\n",
    "tagger_scores['p3ubt'] = evaluateTagger(p3ubt_tagger, test_sents)\n",
    "\n",
    "#UBT, prefix-suffix tagger\n",
    "s3p3ubt_tagger =  nltk.tag.DefaultTagger('NN')\n",
    "s3p3ubt_tagger._taggers = p3ubt_tagger._taggers + suffix3_tagger._taggers\n",
    "print('\\nSuffix3-Prefix3-UBT:')\n",
    "tagger_scores['s3p3ubt'] = evaluateTagger(s3p3ubt_tagger, test_sents)\n",
    "\n",
    "#Regex tagger\n",
    "word_patterns = [\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'(ter|se|bi)[a-z]*', 'JJ'),\n",
    "    (r'ke[a-z]*an', 'JJ'),\n",
    "\n",
    "    (r'me(ny|ng|r|l|w|y|p|t|k|s)[a-z]*', 'VBI'),\n",
    "    (r'mem(b|f|p|v)[a-z]*(kan|i)?', 'VBI'),\n",
    "    (r'men(d|c|j|sy|z|t|s)[a-z]*(kan|i)?', 'VBI'),\n",
    "    (r'meng(g|gh|kh|h|k|a|e|i|o|u)[a-z]*', 'VBI'),\n",
    "    (r'menge[a-z]*(an)?', 'VBI'),\n",
    "    (r'(mem|di)per[a-z]*(kan)?', 'VBI'),\n",
    "    (r'ber[a-z]*(kan|an)?', 'VBI'),\n",
    "    (r'ter[a-z]*', 'VBI'),\n",
    "    (r'ke[a-z]*(an)?', 'VBI'),\n",
    "    (r'di(per)?[a-z]*(kan|i)?', 'VBI'),\n",
    "    \n",
    "    (r'.*nya$', 'NNG')\n",
    "    \n",
    "]\n",
    "\n",
    "regex_tagger = nltk.tag.RegexpTagger(word_patterns)\n",
    "rs3p3ubt_tagger =  nltk.tag.DefaultTagger('NN')\n",
    "rs3p3ubt_tagger._taggers = s3p3ubt_tagger._taggers + regex_tagger._taggers\n",
    "print('\\nRegex-Suffix3-Prefix3-UBT:')\n",
    "tagger_scores['rs3p3ubt'] = evaluateTagger(rs3p3ubt_tagger, test_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use data viz here\n",
    "for taggerName, tagger in seq_taggers.items():\n",
    "    print(\"\\n\"+ taggerName)\n",
    "    evaluateTagger(tagger, test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hidden Markov Model POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmm_trainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_trainer.train_supervised(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8155819281944399\n",
      "Recall: 0.710136155939439\n",
      "Accuracy: 0.710136155939439\n",
      "Precision: 0.9833169560956014\n"
     ]
    }
   ],
   "source": [
    "tagger_scores['hmm'] = evaluateTagger(hmm_tagger, test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Custom Naive bayes classifier based tagger\n",
    "\n",
    "class custom_naiveBayesPOSTagger(nltk.tag.sequential.ClassifierBasedTagger):\n",
    "    \"\"\"\n",
    "    A naive Bayes classifier based part of speech tagger.\n",
    "    \"\"\"\n",
    "\n",
    "    def feature_detector(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        if index == 0:\n",
    "            prevword = prevprevword = \"\"\n",
    "            prevtag = prevprevtag = \"\"\n",
    "        elif index == 1:\n",
    "            prevword = tokens[index-1].lower()\n",
    "            prevprevword = \"\"\n",
    "            prevtag = history[index-1]\n",
    "            prevprevtag = \"\"\n",
    "        else:\n",
    "            prevword = tokens[index-1].lower()\n",
    "            prevprevword = tokens[index-2].lower()\n",
    "            prevtag = history[index-1]\n",
    "            prevprevtag = history[index-2]\n",
    "\n",
    "        if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "            shape = 'number'\n",
    "        elif re.match('\\W+$', word):\n",
    "            shape = 'punct'\n",
    "        elif re.match('[A-Z][a-z]+$', word):\n",
    "            shape = 'upcase'\n",
    "        elif re.match('[a-z]+$', word):\n",
    "            shape = 'downcase'\n",
    "        elif re.match('\\w+$', word):\n",
    "            shape = 'mixedcase'\n",
    "        else:\n",
    "            shape = 'other'\n",
    "\n",
    "        features = {\n",
    "            'prevtag': prevtag,\n",
    "            'prevprevtag': prevprevtag,\n",
    "            'word': word,\n",
    "            'word.lower': word.lower(),\n",
    "            'suffix3': word.lower()[-3:],\n",
    "            'suffix2': word.lower()[-2:],\n",
    "            'suffix1': word.lower()[-1:],\n",
    "            'prefix3': word.lower()[:3],\n",
    "            'prefix2': word.lower()[:2],\n",
    "            'prefix1': word.lower()[:1],\n",
    "            'prevprevword': prevprevword,\n",
    "            'prevword': prevword,\n",
    "            'prevword+suffix3': prevword[-3:],\n",
    "            'prevword+prefix3': prevword[:3],\n",
    "            'prevtag+word': '%s+%s' % (prevtag, word.lower()),\n",
    "            'prevprevtag+word': '%s+%s' % (prevprevtag, word.lower()),\n",
    "            'prevword+word': '%s+%s' % (prevword, word.lower()),\n",
    "            'shape': shape,\n",
    "            }\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.926375523133134\n",
      "Recall: 0.9241001094976506\n",
      "Accuracy: 0.9241001094976506\n",
      "Precision: 0.9420780777498295\n"
     ]
    }
   ],
   "source": [
    "naiveBayes_tagger = custom_naiveBayesPOSTagger(train=train_sents)\n",
    "tagger_scores['naive_bayes'] = evaluateTagger(naiveBayes_tagger, test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Perceptron Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class PerceptronTagger_custom(nltk.tag.perceptron.PerceptronTagger):\n",
    "    \n",
    "    def _get_features(self, i, word, context, prev, prev2):\n",
    "        '''Map tokens into a feature representation, implemented as a\n",
    "        {hashable: int} dict. If the features change, a new model must be\n",
    "        trained.\n",
    "        '''\n",
    "        def add(name, *args):\n",
    "            features[' '.join((name,) + tuple(args))] += 1\n",
    "\n",
    "        i += len(self.START)\n",
    "        features = defaultdict(int)\n",
    "        # It's useful to have a constant feature, which acts sort of like a prior\n",
    "        add('bias')\n",
    "        add('i suffix', word[-3:])\n",
    "        add('i pref3', word[:3])\n",
    "        add('i pref2', word[:2])\n",
    "        add('i-1 tag', prev)\n",
    "        add('i-2 tag', prev2)\n",
    "        add('i tag+i-2 tag', prev, prev2)\n",
    "        add('i word', context[i])\n",
    "        add('i-1 tag+i word', prev, context[i])\n",
    "        add('i-1 word', context[i-1])\n",
    "        add('i-1 suffix', context[i-1][-3:])\n",
    "        add('i-1 prefix', context[i-1][:3])\n",
    "        add('i-2 word', context[i-2])\n",
    "        add('i+1 word', context[i+1])\n",
    "        add('i+1 suffix', context[i+1][-3:])\n",
    "        add('i+1 prefix', context[i-1][:3])\n",
    "        add('i+2 word', context[i+2])\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_mean: 0.9988696352059483\n",
      "F1-Score_mean: 0.9988670324399808\n",
      "Accuracy_mean: 0.9988692689529073\n",
      "Recall_mean: 0.9988692689529073\n"
     ]
    }
   ],
   "source": [
    "perceptron_custom = PerceptronTagger_custom(load = False)\n",
    "scores = cross_val(perceptron_custom, sents_tagged, n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The averaged perceptron tagger has the best performance, with an F-score and average 10-fold cross validation accuracy of 99.89%, beating the benchmark of [97.57%](http://www.panl10n.net/english/outputs/Indonesia/UI/0901/UI-POSTAG.pdf) averaged accuracy trained on the same PANL10N corpus. Let's try making more changes to our model that will allow it to generalise better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce morphosyntatic distinctions\n",
    "\n",
    "The tagged corpus was tagged with a tagset of 37 tags built for the Indonesian language. Let's have a look at the tag distribution of our training corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UH': 9, 'CC': 17894, 'NNG': 2655, 'CDO': 1455, 'PRN': 124, 'CDC': 46, 'NEG': 4965, ':': 426, 'RB': 16705, 'VB': 90, 'MD': 11330, 'NNC': 46856, 'SYM': 1285, 'RP': 288, 'CDI': 3286, '--': 539, 'WRB': 648, 'PRP': 8760, 'FW': 983, 'SC': 30669, '.': 29237, 'VBI': 18784, 'NNP': 8428, 'JJ': 23359, 'NN': 266937, 'VBT': 36291, 'DT': 15632, 'NNS': 852, 'PRL': 198, 'WP': 264, 'IN': 56048, ',': 35549, 'CDP': 15378}\n"
     ]
    }
   ],
   "source": [
    "#Replace morphosyntatic distinction-- don't need that much complexity.\n",
    "#VBI, VBT ==> VB. Reduce number of classes.\n",
    "#Tagging unknown words to better generalise to out of vocabulary words\n",
    "def genTagDist(trainSet):\n",
    "    '''\n",
    "    Generates distribution of tags within training set\n",
    "    '''\n",
    "    tags = genTagset(trainSet)\n",
    "    tagset = dict(zip(tags, [0]*len(tags)))\n",
    "    for i in range(len(trainSet)):\n",
    "        for j in range(len(trainSet[i])):\n",
    "            tagset[trainSet[i][j][1]] += 1\n",
    "    \n",
    "    return tagset\n",
    "\n",
    "tagDist = genTagDist(train_sents)\n",
    "print(tagDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the tags for verbs, there's a clear spread between VBI (intransitive verbs), VBT (transitive verbs), and VB (which is not in the official Indonesian tagset and must have resulted from mistaggings).\n",
    "By grouping these similar POS distinctions into a single POS tag, we can reduce number of classes/tags that our tagger will have to learn, and increase the overall classification accuracy as the errors from misclassified samples amongst these similar groupings would be reduced.\n",
    "This will also allow our model to generalise better to unseen data.\n",
    "\n",
    "A separate script was written to do so, with the following changes:\n",
    "\n",
    "- Group all cardinal distinctions into one:\n",
    "        CDC, CDI, CDO, CDP ==> CD\n",
    "- Group all verb distinctions into one:\n",
    "        VB, VBI, VBT ==> VB\n",
    "- Group NN(common nouns) with NNC (countable common nouns) and NNP (uncountable common nouns). Retain distinction between NNP (proper nouns) and NNG (genitive common nouns) as these would be useful for Named Entity Recognition for a future project:\n",
    "        NN, NNC, NNP ==> NN\n",
    "- Remove tags such as UH (interjections, eg. 'Wah', 'Aduh') which don't have enough samples to properly learn from. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_pos_reduced = pickle.load(open('../data/tagged/malay_pos_reduced.p', 'rb'))\n",
    "perceptron_reduced = PerceptronTagger_custom(load = False)\n",
    "perceptron_reduced.train(sents_pos_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_mean: 0.9986768201147038\n",
      "F1-Score_mean: 0.9986760702516395\n",
      "Accuracy_mean: 0.9986762742309307\n",
      "Recall_mean: 0.9986762742309307\n"
     ]
    }
   ],
   "source": [
    "perceptron_red_scores = cross_val(perceptron_reduced, sents_pos_reduced, n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the average F1-score of 99.867% for our model with reduced tags is slightly lower than our previous model trained on all 37 tags, it's likely to generalise better to out of sample data which contain different vocabularies unseen from our training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our training corpus\n",
    "\n",
    "A look through our training corpus reveals some misclassifications, such as words that are clearly verbs being tagged as nouns. This was a parallel tagged Indonesian-English corpus, and it appears that some english sentences seem to have made their way into the dataset as well. A separate script was run to make the following changes to our corpus:\n",
    "\n",
    "- Fix verb mistags\n",
    "- Fix misclassified symbols\n",
    "- Fix misclassified numeric tokens\n",
    "- Remove English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents_pos_improved = pickle.load(open('../data/tagged/malay_pos_improved.p', 'rb'))\n",
    "perceptron_improved = PerceptronTagger_custom(load = False)\n",
    "perceptron_improved.train(sents_pos_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_mean: 0.9986826182539877\n",
      "Precision_mean: 0.9986830746117807\n",
      "Recall_mean: 0.9986826182539877\n",
      "F1-Score_mean: 0.9986824350854537\n"
     ]
    }
   ],
   "source": [
    "improved_score = cross_val(perceptron_improved, sents_pos_improved, n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The F1 score of our model built on the improved training corpus is slightly higher than our previous model, which shows we're on the right track. \n",
    "Before we get too excited about our positive scores so far which have outperformed our benchmark, we need to assess how well our model generalises to out of sample data from different corpuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluating on out of sample data of different corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OOS = open('../data/idn-tagged-corpus/Indonesian_Manually_Tagged_Corpus_ID.tsv').read() #From https://github.com/famrashel/idn-tagged-corpus\n",
    "sents = re.findall(r'<kalimat id=[0-9]+>((.|\\n)*?)</kalimat>', OOS, re.DOTALL)\n",
    "#NLTK formatted\n",
    "OOS_sents = [[tuple(pair.split('\\t')) for pair in sent[0].strip().split('\\n')] for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we evaluate our model on this new corpus, let's take a look at some tags in the manually tagged corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CC', 'RB', 'Z', 'MD', 'SYM', 'RP', 'SC', 'NNP', 'JJ', 'IN', 'DT', 'CD', 'UH', 'X', 'FW', 'NEG', 'VB', 'OD', 'PRP', 'NND', 'NN', 'PR', 'WH'}\n"
     ]
    }
   ],
   "source": [
    "def genTagset(trainSet):\n",
    "    tagset = set([trainSet[i][j][1] for i in range(len(trainSet)) for j in range(len(trainSet[i]))])\n",
    "    return tagset\n",
    "\n",
    "print(genTagset(OOS_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some new tags such as RP and PR which our tagger doesn't recognise. We'll have to do some preprocessing on this new tagged corpus to be able to properly evaluate the performance of our model, and avoid deceitfully low scores from tagset incompatibility, as follows:\n",
    "\n",
    "- **RP**: particles, eg. 'lah', 'kah', 'pun'.\n",
    "        Our tagger doesn't recognise RP. Concatenate with previous word.\n",
    "- **PR**: eg. 'itu', 'ini'.\n",
    "        Replace this with the equivalent determiner tag(DT) that our tagger recognises.\n",
    "- **NND**: eg. sebuah, barel, kiloliter.\n",
    "        Appears to capture units of measurement. Replace with NN.\n",
    "- **UH**: interjections, eg. 'ya', 'wah', 'aduh', 'oh'.\n",
    "        Only a miniscule part of test set, remove.\n",
    "- **Z**: symbols.\n",
    "        Replace to match our tagset of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combineParticles(sents):\n",
    "    for i in range(len(sents)):\n",
    "        sent = sents[i]\n",
    "        popList = []\n",
    "        for j in range(len(sent)-1):\n",
    "            token = sent[j]\n",
    "            tokenNext = sent[j+1]\n",
    "            if tokenNext[1]=='RP':\n",
    "                sents[i][j] = (token[0]+tokenNext[0], token[1])\n",
    "                popList.append(j+1)\n",
    "        for p in popList:\n",
    "            sents[i].pop(p)\n",
    "            \n",
    "combineParticles(OOS_sents)\n",
    "\n",
    "def fixSymbol(token, tag):\n",
    "    if tag not in [',', '.', ':', '--']:\n",
    "        if token in ['?', '.', '!']: #sentence terminators\n",
    "            return (token, '.')\n",
    "        elif token in ['\"', '(', ')']:\n",
    "            return (token, token)\n",
    "        else:\n",
    "            return (token, 'SYM')\n",
    "    else: \n",
    "        return (token, tag)\n",
    "\n",
    "#OOS_sents_fixed = [[fixSymbol(pos_pair[0], pos_pair[1]) if pos_pair[1]=='Z' else (pos_pair[0], pos_pair[1]) for pos_pair in sent] for sent in OOS_sents]\n",
    "OOS_sents_fixed = [[(pos_pair[0], pos_pair[0]) if pos_pair[1]=='Z' else (pos_pair[0], pos_pair[1]) for pos_pair in sent] for sent in OOS_sents]\n",
    "OOS_sents_fixed = [[(pos_pair[0], 'DT') if pos_pair[1]=='PR' else (pos_pair[0], pos_pair[1]) for pos_pair in sent] for sent in OOS_sents_fixed]\n",
    "OOS_sents_fixed = [[(pos_pair[0], pos_pair[1]) for pos_pair in sent if pos_pair[1]!='UH' or pos_pair[1]!='X' or pos_pair[1]!='RP'] for sent in OOS_sents_fixed]\n",
    "OOS_sents_fixed = [[(pos_pair[0], 'NN') if pos_pair[1]=='NND' else (pos_pair[0], pos_pair[1]) for pos_pair in sent] for sent in OOS_sents_fixed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron trained on reduced tags: \n",
      "Accuracy: 0.7582485340957738\n",
      "Precision: 0.7740084055846063\n",
      "F1-Score: 0.7433275835319221\n",
      "Recall: 0.7582485340957738\n",
      "\n",
      "Perceptron trained on reduced tags and improved corpus: \n",
      "Accuracy: 0.7670498505622081\n",
      "Precision: 0.7655742792011256\n",
      "F1-Score: 0.7530183093037409\n",
      "Recall: 0.7670498505622081\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron trained on reduced tags: \")\n",
    "perceptron_red_scores = evaluateTagger(perceptron_reduced, OOS_sents_fixed)\n",
    "print(\"\\nPerceptron trained on reduced tags and improved corpus: \")\n",
    "perceptron_improved_scores = evaluateTagger(perceptron_improved, OOS_sents_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements\n",
    "\n",
    "The actual Malay POS tagger in this project was trained on a version of the PANL1ON and manually tagged corpuses adapted to the Malay language, in addition to some semi-manually tagged parliamentary documents from Sinar's Pardocs for a final accuracy of .\n",
    "\n",
    "Further improvements that could be made:\n",
    "- Train additionally on words from the Malay Wordnet.\n",
    "- Train model on corpus that splits on genitive common nouns (-nya, -punya), and particles (-lah, -kah). Would have to write corresponding preprocessor."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
